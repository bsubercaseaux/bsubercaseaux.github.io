%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Bernardo Roa at 2023-01-23 14:25:13 -0800 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{bpms2020,
	abstract = {In spite of several claims stating that some models are more interpretable than others -- e.g., "linear models are more interpretable than deep neural networks" -- we still lack a principled notion of interpretability to formally compare among different classes of models. We make a step towards such a notion by studying whether folklore interpretability claims have a correlate in terms of computational complexity theory. We focus on local post-hoc explainability queries that, intuitively, attempt to answer why individual inputs are classified in a certain way by a given model. In a nutshell, we say that a class \mathcal{C}_1 of models is more interpretable than another class \mathcal{C}_2, if the computational complexity of answering post-hoc queries for models in \mathcal{C}_2 is higher than for those in \mathcal{C}_1. We prove that this notion provides a good theoretical counterpart to current beliefs on the interpretability of models; in particular, we show that under our definition and assuming standard complexity-theoretical assumptions (such as P\neqNP), both linear and tree-based models are strictly more interpretable than neural networks. Our complexity analysis, however, does not provide a clear-cut difference between linear and tree-based models, as we obtain different results depending on the particular post-hoc explanations considered. Finally, by applying a finer complexity analysis based on parameterized complexity, we are able to prove a theoretical result suggesting that shallow neural networks are more interpretable than deeper ones.},
	arxiv = {2010.12265},
	author = {Pablo Barcel{\'{o}} and Mika{\"{e}}l Monet and Jorge P{\'{e}}rez and Bernardo Subercaseaux},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	bibtex_show = {true},
	biburl = {https://dblp.org/rec/conf/nips/BarceloM0S20.bib},
	booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual},
	editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin},
	pages = {15487--15498},
	pdf = {https://arxiv.org/pdf/2010.12265.pdf},
	publisher = {Curran Associates, Inc.},
	selected = {true},
	timestamp = {Tue, 19 Jan 2021 15:56:49 +0100},
	title = {Model Interpretability through the lens of Computational Complexity},
	url = {https://proceedings.neurips.cc/paper/2020/hash/b1adda14824f50ef24ff1c05bb66faf3-Abstract.html},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2020/hash/b1adda14824f50ef24ff1c05bb66faf3-Abstract.html}}

@inproceedings{sh2023LPAR,
    title={Toward Optimal Radio Colorings of Hypercubes via SAT-solving},
    abstract={Radio 2-colorings of graphs are a generalization of vertex colorings motivated by the problem of assigning frequency channels in radio networks. In a radio 2-coloring of a graph, vertices are assigned integer colors so that the color of two vertices u and v differ by at least 2 if u and v are neighbors, and by at least 1 if u and v have a common neighbor. Our work improves the best-known bounds for optimal radio 2-colorings of small hypercube graphs, a combinatorial problem that has received significant attention in the past. We do so by using automated reasoning techniques such as symmetry breaking and Cube and Conquer, obtaining that for n = 7 and n = 8, the coding-theory upper bounds of Whittlesey et al. (1995) are not tight. Moreover, we prove the answer for n=7 to be either 12 or 13, thus making a substantial step towards answering an open problem by Knuth (2015). Finally, we include several combinatorial observations that might be useful for further progress, while also arguing that fully determining the answer for n=7 will require new techniques.},
    author={Bernardo Subercaseaux and Marijn J. H. Heule},
    booktitle={Proceedings of 24th International Conference on Logic
for Programming, Artificial Intelligence and Reasoning},
    pages={1-19},
    editor={A. Voronkov and R. Piskac},
    year={2023},
    selected={true},
    pdf={https://www.cs.cmu.edu/~mheule/publications/LPAR23.pdf},
    bibtex_show = {true},
}

@inproceedings{sh2023,
  abstract={A packing $k$-coloring is a natural variation on the standard notion of graph $k$-coloring, where vertices are assigned numbers from $\{1, \ldots, k\}$, and any two vertices assigned a common color $c \in \{1, \ldots, k\}$ need to be at a distance greater than $c$ (as opposed to 1, in standard graph colorings). Despite a sequence of incremental work, determining the packing chromatic number of the infinite square grid has remained an open problem since its introduction in 2002. We culminate the search by proving this number to be 15. We achieve this result by improving the best-known method for this problem by roughly two orders of magnitude. The most important technique to boost performance is a novel, surprisingly effective propositional encoding for packing colorings. Additionally, we developed an alternative symmetry-breaking method. Since both new techniques are more complex than existing techniques for this problem, a verified approach is required to trust them. We include both techniques in a proof of unsatisfiability, reducing the trusted core to the correctness of the direct encoding.},
  title = {The Packing Chromatic Number of the Infinite Square Grid is 15},
  author = {Bernardo Subercaseaux and Marijn J. H. Heule},
  arxiv = {2301.09757},
    bibtex_show = {true},
    booktitle = {TACAS (proceedings are not out yet)},
    editor = {},
    pages = {1--20},
    pdf = {https://arxiv.org/pdf/2301.09757.pdf},
    publisher = {arXiv},
    selected = {true},
    volume = {13993},
    bdsk-url-1 = {https://arixv.org/abs/2301.09757},
    series={Lecture Notes in Computer Science}, 
    rights={All rights reserved}, 
    url={https://doi.org/10.1007/978-3-031-30823-9\_20}, 
    DOI={10.1007/978-3-031-30823-9_20}, 
    booktitle={Tools and Algorithms for the Construction and Analysis of Systems - 29th International Conference, TACAS 2023, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2022, Paris, France, April 22-27, 2023, Proceedings, Part I}, 
    publisher={Springer}, 
    author={Subercaseaux, Bernardo and Heule, Marijn J. H.}, 
    editor={Sankaranarayanan, Sriram and Sharygina, Natasha}, 
    year={2023}, pages={389–406}, collection={Lecture Notes in Computer Science} 
}

@article{bps2020,
	abstract = {Despite the growing interest in interpretability for machine learning models, there
seems to be a gap between the world of researchers and that of practitioners and
data scientists. Declarative languages, tailored for interactive interpretability and
bias detection of models, could be a step towards shortening such a gap. Based
on requirements often posed over interpretability tasks, it is desirable for such
languages to be interactive, model agnostic, faithful to the models, and have
clear and well-understood semantics and complexity. A reasonable way to meet
several of these requirements is by having a language strongly rooted in logic. As
a first step towards the design of such language, we study how interpretability
and bias detection queries can be expressed in first-order logic (FO) --arguably,
one of the most prominent logics in different areas of computer science-- over a
suitable encoding of machine learning models. We then study the computational
complexity of evaluating FO formulae over linear-based, tree-based and deep
models. Evaluation is shown to be easier for decision trees and perceptrons than
it is for deep neural networks, which seems validated by folklore assumptions
of the field. Finally, we discuss possible extensions to the underlying logical
structure and how they enhance expressiveness, as well as possible directions
towards tractability.},
	author = {Pablo Barcel{\'{o}} and Jorge P{\'{e}}rez and Bernardo Subercaseaux},
	bibtex_show = {true},
	journal = {AFCI workshop at NeurIPS 2020. Algorithmic Fairness through the Lens of Causality and Interpretability},
	pdf = {../pdf/afci2020.pdf},
	selected = {true},
	title = {Foundations of Languages for Interpretability and Bias Detection},
	year = {2020}}

@inproceedings{bhps2020,
	abstract = {We study the expressive power of the LARA language -- a recently proposed unified model for expressing relational and linear algebra operations -- both in terms of traditional database query languages and some analytic tasks often performed in machine learning pipelines. We start by showing LARA to be expressive complete with respect to first-order logic with aggregation. Since LARA is parameterized by a set of user-defined functions which allow to transform values in tables, the exact expressive power of the language depends on how these functions are defined. We distinguish two main cases depending on the level of genericity queries are enforced to satisfy. Under strong genericity assumptions the language cannot express matrix convolution, a very important operation in current machine learning operations. This language is also local, and thus cannot express operations such as matrix inverse that exhibit a recursive behavior. For expressing convolution, one can relax the genericity requirement by adding an underlying linear order on the domain. This, however, destroys locality and turns the expressive power of the language much more difficult to understand. In particular, although under complexity assumptions the resulting language can still not express matrix inverse, a proof of this fact without such assumptions seems challenging to obtain.},
	arxiv = {1909.11693},
	author = {Pablo Barcel{\'{o}} and Nelson Higuera and Jorge P{\'{e}}rez and Bernardo Subercaseaux},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	bibtex_show = {true},
	biburl = {https://dblp.org/rec/conf/icdt/BarceloH0S20.bib},
	booktitle = {23rd International Conference on Database Theory, {ICDT} 2020, March 30-April 2, 2020, Copenhagen, Denmark},
	doi = {10.4230/LIPIcs.ICDT.2020.6},
	editor = {Carsten Lutz and Jean Christoph Jung},
	pages = {6:1--6:20},
	pdf = {https://arxiv.org/pdf/1909.11693.pdf},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
	selected = {true},
	series = {LIPIcs},
	timestamp = {Thu, 19 Mar 2020 10:24:23 +0100},
	title = {On the Expressiveness of {LARA:} {A} Unified Language for Linear and Relational Algebra},
	url = {https://doi.org/10.4230/LIPIcs.ICDT.2020.6},
	volume = {155},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.4230/LIPIcs.ICDT.2020.6}}

@inproceedings{abrs2022,
	abstract = {Formal XAI (explainable AI) is a growing area that focuses on computing explanations with mathematical guarantees for the decisions made by ML models. Inside formal XAI, one of the most studied cases is that of explaining the choices taken by decision trees, as they are traditionally deemed as one of the most interpretable classes of models. Recent work has focused on studying the computation of sufficient reasons, a kind of explanation in which given a decision tree $T$ and an instance $x$, one explains the decision $T(x)$ by providing a subset $y$ of the features of $x$ such that for any other instance $z$ compatible with $y$, it holds that  $T(z) = T(x)$, intuitively meaning that the features in $y$ are already enough to fully justify the classification of $x$ by $T$. 
It has been argued, however, that sufficient reasons constitute a restrictive notion of explanation. For such a reason, the community has started to study their probabilistic counterpart, in which one requires that the probability of $T(z) = T(x)$ must be at least some value $\delta \in (0, 1]$, where $z$ is a random instance that is compatible with $y$. Our paper settles the computational complexity of $\delta$-sufficient-reasons over decision trees, showing that both (1) finding $\delta$-sufficient-reasons  that are minimal in size, and (2) finding $\delta$-sufficient-reasons that are minimal inclusion-wise, do not admit polynomial-time algorithms (unless P = NP).
   This is in stark contrast with the deterministic case ($\delta = 1$) where inclusion-wise minimal sufficient-reasons are easy to compute. By doing this, we answer two open problems originally raised by Izza et al., and extend the hardness of explanations for Boolean circuits presented by W{\"a}ldchen et al. to the more restricted case of decision trees. On the positive side, we identify structural restrictions of decision trees that make the problem tractable, and show how SAT solvers might be able to tackle these problems in practical settings.},
	arxiv = {https://arxiv.org/abs/2207.12213},
	author = {Marcelo Arenas and Pablo Barcelo and Miguel Romero Orth and Bernardo Subercaseaux},
	bibtex_show = {true},
	booktitle = {Advances in Neural Information Processing Systems, NeurIPS},
	editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	pdf = {https://arxiv.org/pdf/2207.12213.pdf},
	selected = {true},
	title = {On Computing Probabilistic Explanations for Decision Trees},
	url = {https://openreview.net/forum?id=zD65Zdh6ZhI},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=zD65Zdh6ZhI}}

@inproceedings{gpss2022,
	abstract = {The growing body of work in learning-augmented online algorithms studies how online algorithms can be improved when given access to ML predictions about the future. Motivated by ML models that give a confidence parameter for their predictions, we study online algorithms with predictions that are $\epsilon$-accurate: namely, each prediction is correct with probability (at least) $\epsilon$, but can be arbitrarily inaccurate with the remaining probability. We show that even with predictions that are accurate with a small probability and arbitrarily inaccurate otherwise, we can dramatically outperform worst-case bounds for a range of classical online problems including caching, online set cover, and online facility location. Our main results are an $O(\log(1/\varepsilon))$-competitive algorithm for caching, and a simple $O(1/\varepsilon)$-competitive algorithm for a large family of covering problems, including set cover and facility location, with $\epsilon$-accurate predictions.},
	author = {Anupam Gupta and Debmalya Panigrahi and Bernardo Subercaseaux and Kevin Sun},
	bibtex_show = {true},
	booktitle = {Advances in Neural Information Processing Systems, NeurIPS},
	editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
	pdf = {https://openreview.net/pdf?id=HFkxZ_V0sBQ},
	selected = {true},
	title = {Augmenting Online Algorithms with \$$\varepsilon\$$-Accurate Predictions},
	url = {https://openreview.net/forum?id=HFkxZ_V0sBQ},
	year = {2022},
	bdsk-url-1 = {https://openreview.net/forum?id=HFkxZ_V0sBQ}}

@inproceedings{bs2021,
	abstract = {The game of Hangman is a classical asymmetric two player game in which one player, the setter, chooses a secret word from a language, that the other player, the guesser, tries to discover through single letter matching queries, answered by all occurrences of this letter if any. In the Evil Hangman variant, the setter can change the secret word during the game, as long as the new choice is consistent with the information already given to the guesser. We show that a greedy strategy for Evil Hangman can perform arbitrarily far from optimal, and most importantly, that playing optimally as an Evil Hangman setter is computationally difficult. The latter result holds even assuming perfect knowledge of the language, for several classes of languages, ranging from Finite to Turing Computable. The proofs are based on reductions to Dominating Set on 3-regular graphs and to the Membership problem, combinatorial problems already known to be computationally hard.},
	arxiv = {2003.10000},
	author = {J{\'{e}}r{\'{e}}my Barbay and Bernardo Subercaseaux},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	bibtex_show = {true},
	biburl = {https://dblp.org/rec/conf/fun/BarbayS21.bib},
	booktitle = {10th International Conference on Fun with Algorithms, {FUN} 2021, May 30 to June 1, 2021, Favignana Island, Sicily, Italy},
	doi = {10.4230/LIPIcs.FUN.2021.23},
	editor = {Martin Farach{-}Colton and Giuseppe Prencipe and Ryuhei Uehara},
	pages = {23:1--23:12},
	pdf = {https://arxiv.org/pdf/2003.10000.pdf},
	publisher = {Schloss Dagstuhl - Leibniz-Zentrum f{\"{u}}r Informatik},
	selected = {true},
	series = {LIPIcs},
	timestamp = {Mon, 21 Dec 2020 13:23:22 +0100},
	title = {The Computational Complexity of Evil Hangman},
	url = {https://doi.org/10.4230/LIPIcs.FUN.2021.23},
	volume = {157},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.4230/LIPIcs.FUN.2021.23}}

@inproceedings{bhps2019,
	abstract = {Tensors are one of the most widely used data structures in modern Machine Learning applications. Although they provide a flexible way of storing and accessing data, they often expose too many low-level details that may result in error prone code that is difficult to maintain and extend. Abstracting low-level functionalities into high-level operators in the form of a query language is a task in which the Data Management community has extensive experience. It is thus important to understand how such an experience can be applied in the design of useful languages for tensor manipulation. In this short paper we study a matrix and a tensor query language that have been recently proposed in the database literature. We show, by using examples, how these proposals are in line with the practical interest in rethinking tensor abstractions. On the technical side, we compare the two languages in terms of operators that naturally arise in Machine Learning pipelines, such as convolution, matrix-inverse, and Einstein summation. We hope our results to provide a theoretical kick-off for the discussion on the design of core declarative query languages for tensors.},
	author = {Pablo Barcel{\'{o}} and Nelson Higuera and Jorge P{\'{e}}rez and Bernardo Subercaseaux},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	bibtex_show = {true},
	biburl = {https://dblp.org/rec/conf/sigmod/BarceloH0S19.bib},
	booktitle = {Proceedings of the 3rd International Workshop on Data Management for End-to-End Machine Learning, DEEM@SIGMOD 2019, Amsterdam, The Netherlands, June 30, 2019},
	doi = {10.1145/3329486.3329498},
	editor = {Sebastian Schelter and Neoklis Polyzotis and Stephan Seufert and Manasi Vartak},
	html = {https://dl.acm.org/doi/10.1145/3329486.3329498},
	pages = {9:1--9:5},
	pdf = {../pdf/deem19.pdf},
	publisher = {{ACM}},
	selected = {true},
	timestamp = {Wed, 05 Jun 2019 14:54:54 +0200},
	title = {Expressiveness of Matrix and Tensor Query Languages in terms of {ML} Operators},
	url = {https://doi.org/10.1145/3329486.3329498},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1145/3329486.3329498}}

@article{clps2016,
	abstract = {The wavelet tree is a data structure to succinctly represent sequences of elements over
a fixed but potentially large alphabet. It is a very versatile data structure which exhibits interesting properties even when its compression capabilities are not considered, efficiently supporting
several queries. Although the wavelet tree was proposed more than a decade ago, it has not yet
been widely used by the competitive programming community. This paper tries to fill the gap
by showing how this data structure can be used in classical competitive programming problems,
discussing some implementation details, and presenting a performance analysis focused in a
competitive programming setting.},
	author = {Robinson Castro and Nico Lehmann and Jorge P{\'e}rez and Bernardo Subercaseaux},
	bibtex_show = {true},
	doi = {10.15388/ioi.2016.02},
	journal = {Olympiads in Informatics},
	month = jul,
	number = {1},
	pages = {19--37},
	pdf = {https://ioinformatics.org/journal/v10_2016_19_37.pdf},
	publisher = {Vilnius University Press},
	select = {true},
	title = {Wavelet Trees for Competitive Programming},
	url = {https://doi.org/10.15388/ioi.2016.02},
	volume = {10},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.15388/ioi.2016.02}}

@article{abbps2021,
	abstract = {Several queries and scores have been proposed to explain individual predictions
made by ML models. Examples include queries based on ``anchors", which are
parts of an instance that are sufficient to justify its classification, and ''feature perturbation" scores such as SHAP. 
Given the need for flexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We
do this in a principled way by rooting such a language in a logic called FOIL,
that allows for expressing many simple but important explainability queries, and
might serve as a core for more expressive interpretability languages. We study the
computational complexity of FOIL queries over classes of ML models often deemed
to be easily interpretable: decision trees and more general decision diagrams. Since
the number of possible inputs for an ML model is exponential in its dimension,
tractability of the FOIL evaluation problem is delicate, but can be achieved by either
restricting the structure of the models, or the fragment of FOIL being evaluated.
We also present a prototype implementation of FOIL wrapped in a high-level
declarative language, and perform experiments showing that such a language can
be used in practice.},
	archiveprefix = {arXiv},
	arxiv = {2110.02376},
	author = {Marcelo Arenas and Daniel Baez and Pablo Barcel{\'o} and Jorge P{\'e}rez and Bernardo Subercaseaux},
	bibtex_show = {true},
	booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021},
	date-modified = {2023-01-23 14:25:10 -0800},
	eprint = {2110.02376},
	journal = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021},
	pdf = {https://arxiv.org/pdf/2110.02376.pdf},
	primaryclass = {cs.AI},
	publisher = {Curran Associates, Inc.},
	selected = {true},
	title = {Foundations of Symbolic Languages for Model Interpretability},
	url = {https://arxiv.org/abs/2110.02376},
	year = {2021},
	bdsk-url-1 = {https://arxiv.org/abs/2110.02376}}

        

@inproceedings{ls2022,
	abstract = {Wordle is a single-player word-guessing game where the goal is to discover a secret word w that has been chosen from a dictionary D. In order to discover w, the player can make at most 𝓁 guesses, which must also be words from D, all words in D having the same length k. After each guess, the player is notified of the positions in which their guess matches the secret word, as well as letters in the guess that appear in the secret word in a different position. We study the game of Wordle from a complexity perspective, proving NP-hardness of its natural formalization: to decide given a dictionary D and an integer 𝓁 if the player can guarantee to discover the secret word within 𝓁 guesses. Moreover, we prove that hardness holds even over instances where words have length k = 5, and that even in this case it is NP-hard to approximate the minimum number of guesses required to guarantee discovering the secret word. We also present results regarding its parameterized complexity and offer some related open problems.},
	address = {Dagstuhl, Germany},
	annote = {Keywords: wordle, np-hardness, complexity},
	arxiv = {https://arxiv.org/abs/2203.16713},
	author = {Lokshtanov, Daniel and Subercaseaux, Bernardo},
	bibtex_show = {true},
	booktitle = {11th International Conference on Fun with Algorithms (FUN 2022)},
	doi = {10.4230/LIPIcs.FUN.2022.19},
	editor = {Fraigniaud, Pierre and Uno, Yushi},
	isbn = {978-3-95977-232-7},
	issn = {1868-8969},
	pages = {19:1--19:8},
	pdf = {https://arxiv.org/pdf/2203.16713.pdf},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	selected = {true},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	title = {{Wordle Is NP-Hard}},
	url = {https://drops.dagstuhl.de/opus/volltexte/2022/15989},
	urn = {urn:nbn:de:0030-drops-159893},
	volume = {226},
	year = {2022},
	bdsk-url-1 = {https://drops.dagstuhl.de/opus/volltexte/2022/15989},
	bdsk-url-2 = {https://doi.org/10.4230/LIPIcs.FUN.2022.19}}

@inproceedings{sh2022,
	abstract = {A packing k-coloring of a graph G = (V, E) is a mapping from V to {1, ..., k} such that any pair of vertices u, v that receive the same color c must be at distance greater than c in G. Arguably the most fundamental problem regarding packing colorings is to determine the packing chromatic number of the infinite square grid. A sequence of previous works has proved this number to be between 13 and 15. Our work improves the lower bound to 14. Moreover, we present a new encoding that is asymptotically more compact than the previously used ones.},
	address = {Dagstuhl, Germany},
	annote = {Keywords: packing coloring, SAT solvers, encodings},
	author = {Subercaseaux, Bernardo and Heule, Marijn J.H.},
	bibtex_show = {true},
	booktitle = {25th International Conference on Theory and Applications of Satisfiability Testing (SAT 2022)},
	doi = {10.4230/LIPIcs.SAT.2022.21},
	editor = {Meel, Kuldeep S. and Strichman, Ofer},
	isbn = {978-3-95977-242-6},
	issn = {1868-8969},
	pages = {21:1--21:16},
	pdf = {https://drops.dagstuhl.de/opus/volltexte/2022/15989/pdf/LIPIcs-FUN-2022-19.pdf},
	publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
	selected = {true},
	series = {Leibniz International Proceedings in Informatics (LIPIcs)},
	title = {{The Packing Chromatic Number of the Infinite Square Grid Is at Least 14}},
	url = {https://drops.dagstuhl.de/opus/volltexte/2022/16695},
	urn = {urn:nbn:de:0030-drops-166951},
	volume = {236},
	year = {2022},
	bdsk-url-1 = {https://drops.dagstuhl.de/opus/volltexte/2022/16695},
	bdsk-url-2 = {https://doi.org/10.4230/LIPIcs.SAT.2022.21}}


@misc{arenas2023symbolic,
      title={A Symbolic Language for Interpreting Decision Trees}, 
      author={Marcelo Arenas and Pablo Barcelo and Diego Bustamente and Jose Caraball and Bernardo Subercaseaux},
      year={2023},
      eprint={2310.11636},
      booktitle={Preprint},
      pdf={https://arxiv.org/pdf/2310.11636.pdf},
      selected = {true},
      archivePrefix={arXiv},
	  bibtex_show = {true},
      primaryClass={cs.LO}
}


@misc{subercaseaux2023minimizing,
  title         = {Minimizing Pentagons in the Plane through Automated Reasoning},
  abstract      = {We study {$\mu_5(n)$}, the minimum number of convex pentagons induced by {$n$} points in the plane in general position. Despite a significant body of research in understanding {$\mu_4(n)$}, the variant concerning convex quadrilaterals, not much is known about {$\mu_5(n)$}. We present two explicit constructions, inspired by point placements obtained through a combination of Stochastic Local Search and a program for realizability of point sets, that provide {$\mu_5(n) \leq {{\lfloor n/2\rfloor} \choose 5} + { {\lceil n/2\rceil} \choose 5}$}. Furthermore, we conjecture this bound to be optimal, and provide partial evidence by leveraging a MaxSAT encoding that allows us to verify our conjecture for {$n \leq 16$}.}
  author        = {Bernardo Subercaseaux and John Mackey and Marijn J. H. Heule and Ruben Martins},
  year          = {2023},
  bibtex_show = {true},
  selected = {true},
  booktitle={Preprint},
  pdf={https://arxiv.org/pdf/2311.03645.pdf},
  eprint        = {2311.03645},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CG}
}

@misc{gutierrez2024assortment,
	  abstract={Conferences such as FUN with Algorithms routinely buy goodies (e.g., t-shirts, coffee mugs, etc) for their attendees. Often, said goodies come in different types, varying by color or design, and organizers need to decide how many goodies of each type to buy. We study the problem of buying optimal amounts of each type under a simple model of preferences by the attendees: they are indifferent to the types but want to be able to choose between more than one type of goodies at the time of their arrival. The indifference of attendees suggests that the optimal policy is to buy roughly equal amounts for every goodie type. Despite how intuitive this conjecture sounds, we show that this simple model of assortment optimization is quite rich, and even though we make progress towards proving the conjecture (e.g., we succeed when the number of goodie types is 2 or 3), the general case with K types remains open. We also present asymptotic results and computer simulations, and finally, to motivate further progress, we offer a reward of $100usd for a full proof. }
      title={Assortment Optimization For Conference Goodies With Indifferent Attendees}, 
      author={Fernanda Guti\'errez and Bernardo Subercaseaux},
      year={2024},
      pdf={https://arxiv.org/pdf/2403.03330.pdf},
      booktitle={Preprint},
      selected = {true},
	  bibtex_show = {true},
      eprint={2403.03330},
      archivePrefix={arXiv},
      arxiv={2403.03330},
      primaryClass={math.OC}
}

@article{subercaseauxHeuleNAW,
	abstract= {Almost identical mathematical problems can result in wildly different solutions. In this
	piece of counterfactual history, Bernardo Subercaseaux and Marijn Heule discuss two
	graph theoretic problems that Leonhard Euler did not have the opportunity to meet. While
	the proof for the first problem can be elegantly summarized into a paragraph, thanks to
	Euler’s results, the shortest proof known for the second problem uses over 30 terabytes
	of data. Therefore, we argue that despite his mathematical genius, Euler would not have
	been able to solve the latter problem. The almost alien prolificity of the Swiss mathema-
	tician makes him a perfect fictitious intermediary for discussing the divide between the
	world of human mathematics and the world of computational mathematics.},
	author={Bernardo Subercaseaux and Marijn J. H. Heule},
	title= {A proof long enough to stump Leonhard Euler},
	year={2024},
	selected= {true},
	bibtex_show = {true},
	pdf = {../pdf/NAW2024.pdf},
	journal = {Nieuw Archief voor Wiskunde (NAW)}
}
